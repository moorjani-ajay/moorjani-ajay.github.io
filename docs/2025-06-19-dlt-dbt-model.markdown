---
layout: post
title: "Podcasts, Pipelines & Chill ğŸ§"
# date: 2025-06-19
nav_order: 1
# permalink: /posts/podcasts-pipelines-chill/
---

# Podcasts, Pipelines &Â ChillÂ ğŸ§ğŸ› ï¸

I was scrolling Reddit on `r/dataengineering` on a lazy Saturday and found a post about podcast analytics. It was really hot that day ğŸ”¥

so I thought, why not wire up a **tiny** data platform to see the numbers for myself?

---

## Stack TL;DR

| Layer         | Tool               | Why I picked it                                          |
| ------------- | ------------------ | -------------------------------------------------------- |
| Orchestration | **Apache Airflow** | The deâ€‘facto â€œcronâ€‘onâ€‘steroidsâ€.                         |
| Ingest        | **dlt**            | Point â†’ click â†’ data in Snowflake with almost no config. |
| Transform     | **dbt**            | SQL + version control + tests = â¤ï¸                       |
| Warehouse     | **Snowflake**      | Handles scale so I donâ€™t have to.                        |
| Glue          | **Docker Compose** | One `up` and everything runs.                            |
| Meta DB       | **Postgres**       | Airflow needs somewhere to write.                        |

---

## Highâ€‘Level Diagram

![Overall architecture](/assets/images/global_assignment_architecure.jpg)

---

## Warehouse Layers

![Model lineage](/assets/images/global_data_warehouse_layers.png)

- **Bronze**Â â†’ Raw dumps from `dlt`
- **Silver**Â â†’ Clean tables (`int__*`)
- **Gold**Â â†’ Facts & marts (`fct__*`, `mart__*`)

---

## The DAG

1. **Load** â€“ `dlt` pulls the podcast API and lands rows in Snowflake.
2. **Transform** â€“ `dbt run` cleans everything up.
3. **Test** â€“ `dbt test` makes sure I didnâ€™t break anything.

All wrapped in a single file: `podcast_dag.py`.

---

## Running It Yourself

```bash
git clone <thisâ€‘repo>
cd <thisâ€‘repo>
docker-compose up --build      # grab a coffee â˜•
```

Open Airflow at `http://localhost:8080`, enable **podcast_dag**, hit _Trigger_, watch the logs fly.

---

## Quick Peek at a Goldâ€‘Layer Query

```sql
SELECT
  ep.title,
  COUNT(*) AS completed_plays
FROM {{ ref('fct__completed') }}
JOIN {{ ref('int__episodes') }} ep USING (episode_id)
GROUP BY 1
ORDER BY completed_plays DESC
LIMIT 10;
```

Boom â€“ topâ€‘played episodes in one query.

---

## What I Liked

- ğŸï¸ **Speed** â€“ `dlt` + Snowflake moved ~50â€¯k rows in seconds.
- ğŸ” **Transparency** â€“ dbt lineage graph shows exactly where data flows.
- ğŸ›ï¸ **Control** â€“ Airflow UI lets me replay any task with a click.

## What Could Be Better

- The local Airflow image takes a minute to build.
- Secrets in `docker-compose.yaml` are fine for a demo, but use a vault in prod.

---

Thatâ€™s all â€“ a weekend well spent and a nice little pipeline to show for it.  
Questions? Suggestions? Hit me up or find out more about me on my [about page](/about).
